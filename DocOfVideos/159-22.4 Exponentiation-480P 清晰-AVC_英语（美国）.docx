Let's look at an example of a function we can  define in two different ways. One of which is   much more efficient than the other.  Exponentiation is built into Python,   I can raise one number to the power of another.  But let's write an exponentiation function   recursively. Our goal will be to have it so  that one more multiplication lets us double   the exponent that we're using an exponentiation.  Here's an implementation that does not meet this   goal. This says to raise b to the power n if n  is zero, return 1, otherwise, multiply b to the   n-1 by b. This is a correct definition. But if I  double n, I have to do twice as many multiplies.   Because each time I make a recursive call,  I'm only reducing n by 1. And this function's   implementation corresponds to the following  mathematical definition for b to the n. But   this is not the only mathematical definition of  b to the n. Here's another one, more complicated,   but the additional complexity will also give us  additional efficiency. This says the b to the n   is 1 if n is 0. And then has two different cases  for whether n is even or odd. If n is even, then   I cannot raise b to the 1/2 n power and square  that result. So if n is 16, I raise b to the 8,   and then I square b to the 8 to get b to the  16. If n is odd, then I use the definition I had   before. Here's an implementation. This is a more  efficient version of exponentiation that has two   different recursive cases, depending on whether  n is even, in which case you square the result of   exponentiating b and n divided by 2, where I know  that n is even, so floor division will give me the   exact result. And if n is odd, then I use my old  definition. This is not a tree recursive function,   even though I see two different calls to x fast  in the body, only one of them is ever called.   So this is an instance of linear recursion. And  sometimes, the problem size is reduced by half.   Meaning one more multiplication, in this case  by calling square, lets us double the size of   the problem that we're handling. And x fast runs  faster. Let's take a look. This environment is   called a Jupyter Notebook. You can read about them  online. It's a common way that people use in order   to execute Python code when the output is a graph  or a chart. So here's our definition of x the slow   way. And something that's built into this notebook  environment is that we can time how long it takes   to run something. So if I raise 2 to the 400th  power, the time it took was two milliseconds. If   I raised 2 to the 100th power, it only took me 100  microseconds. But if I run it again, I'll get a   different number each time. So if I run it a bunch  of times I see it hovers around 100 microseconds,   2 to the 400th power tends to hover around 200  microseconds. Running it a bunch of times this   way is inefficient. So I've written some code to  run it a bunch of times using the built-in timeit   module, which can repeatedly call something.  And in this case, we're going to compute the   median time that it takes to execute some line of  code. And here's the result. 2 to the 200th power   takes a little less than half a millisecond. 2  to the 400th power takes a little less than one   millisecond. This looks like a straight line.  The term we use to describe this is linear. The   time that it takes to compute 2 to the n under  this implementation is linear in n. Now, it's a   little bit bumpy because all kinds of interesting  things are happening inside of a computer. But   the word linear is used, even if there are slight  deviations from the line to describe the general   trend that appears when you plot the time it  takes to compute something as a function of   the input size. And here's the fast definition.  We can draw the same plot and we'll see that   the overall numbers are quite a bit smaller.  For 1600, instead of taking 4 milliseconds,   it takes only 0.04 milliseconds. Also, the shape  of the curve is different. This shape is called a   logarithmic. The difference between going from  200 to 400, in this case, is just about 0.002.   And what about the difference between going from  400 to 800? Well, that's another 0.002. Going   from 800 to 1600 is another 0.002. Every time we  double the input, it just takes a constant amount   of extra work. And that's because we're making  recursive calls on n that's only half the size.   That's called logarithmic time. So to summarize,  a linear time function requires one more unit of   work for every one bigger n. That means doubling  the input doubles the time. And a thousand times   the input takes a thousand times as much time.  1024 is 2 to the 10th power. Logarithmic time is   much better. Doubling the input just increases  the time by some constant. And a thousand times   the input just increases the time by 10 times  that constant. So it takes 10 times as long to   do thousand times as much work. And these terms  linear and logarithmic are used to describe the   general shape of time as a function of input size,  even if there are wiggles and bumps along the way.
