The order of growth of a function's time that it  takes to compute its result is a general category   that the function falls in, such as linear or  logarithmic. Two functions that use linear time   might not take exactly the same amount of time  to compute their result for the same input,   but they both scale in the same way. The  general shape of their scaling curve is   both a line. And logarithmic functions all have  that same characteristic logarithmic shape. And   there are other shapes as well. Quadratic time  describes a function that processes all pairs   of elements in a linear input are all pairs of  values in a sequence of length n. So for example,   here's a function that computes the overlap  between the elements in a and the elements in b,   by going through every element in a and every  element in b and checking to see if they're the   same. It adds a count when they are and returns  the count at the end. So the overlap between 3,   5, 7, 6 and 4, 5, 6, 5 is 3 because 5, 5 are the  same, 5, 5 are the same, and 6, 6 are the same.   To compute this result, overlap compares all  pairs of an element in a and an element in b.   And that's a quadratic amount of work based on  the input length. Here, the input length is 4,   and the amount of comparisons is 4 times 4 or  16. The amount of extra work required to process   a length of 4 versus a length of 3 is this whole  row. And therefore, if you compare the input size   to the time it takes to compute the result, you  get a quadratic shape. So here's what happens when   we pass in n from the range 20 to 200 and compute  the median time required in order to return the   result. 75 takes 2 milliseconds, whereas 150 takes  nearly 8 milliseconds, and the curve slopes upward   in a familiar-looking parabola. Another upward  sloping order of growth is exponential time. Here,   things are even worse. A tree recursive function  can take exponential time because the unmemoized   fib function has to do 60% more work just to  compute n that's one larger than the previous   n. So computing fib(2) just requires this much  work. Fib(3) requires all this additional work.   Fib(4) requires all this additional work on top of  fib(3). And fib(5) has to compute not only fib(4),   but all of this additional work as well, assuming  we don't memoize. So a tree recursive function   like this without memoization will take what's  called exponential time. Here are the common   orders of growth that arise when analyzing  functions. There are others. And there's also a   formal system for analyzing functions and proving  what orders of growth they belong to. But we'll   defer all of that for a later course. And instead,  focus on understanding what these categories are   and what their implications are on the runtime of  a program. All of them describe how time scales   with input size. And all of them describe general  trends, as opposed to the details of exactly how   many microseconds it takes for a function call  to return. So exponential growth is really slow.   Quadratic growth is slow but common. Linear growth  is very common. Logarithmic growth is great and   scales to really large inputs. And constant growth  is the best of all, that happens when the input   size doesn't affect the time. For example, in  Python, the number of elements in a dictionary   does not affect how long it takes to look up a  value by its key. How that works is a topic for   a later course, but the fact that it is a constant  time operation to find a value by its key is an   important property of Python dictionaries. Each  of these categories can be described by a simple   equation, where we've written down the general  time that it takes to process an input of size   n+1 in terms of the time that it takes to process  an input of size n, using some constants a and b,   to account for the fact that there are many  details about how long it takes for something to   run that we're glossing over as we talk about the  general shape of time as a function of the input   size. So for exponential growth, the amount of  time it takes to compute the results for an input   of size n+1 is something like a * b to the n+1,  which compared to the time it takes for an input   of size n requires an additional multiplicative  factor of b. So that's the key characteristic of   exponential growth. Incrementing the input size n  multiplies the time by some constant. The bigger   the constant, the worse this is. But basically,  for any constant, it's pretty bad. Quadratic   growth is not quite so slow. Quadratic growth says  that for an input of size n+1, it takes something   like a * n+1 squared, in order to compute the  result. Which in addition to the time it takes   to compute the result for size n, also requires  an additional additive term. Now, adding to the   time is better than multiplying the time. But  still, the problem with quadratic growth is that   the amount you add for just going from n to n+1 is  that term that depends on n. So incrementing and   increases time by n times the constant. Linear  growth for a problem of size n+1 requires n+1   times some a steps, which compared to solving the  same problem for size n, it's just an additive   factor that doesn't depend on n. Incrementing n  increases time by a constant. Logarithmic growth   is the one that scales to really big n. Here, we  write down an expression for the time it takes   to compute the result for n+n, or 2 * n is some  constant times the log of 2 n, which compared to   the amount of time it takes to compute the result  for just n, adds some constant. So doubling n only   increments time by a constant. If it's possible  to re-implement some function in order to change   its order of growth, from exponential to linear  by memoization, or from linear to logarithmic as   we did for exponentiation, that can substantially  change not only the speed of your program but the   size of problem that your program can handle,  without seeming like it's taking forever.
