There are precise mathematical definitions for  the terms linear, constant, quadratic, and so   on. And these definitions are useful because they  allow you to prove properties of functions without   ever running those functions on particular  examples. And one important component of the   theory of computing is this practice of proving  characteristics about functions, by reasoning from   mathematical definitions. Now that's not a topic  in this course, but it is covered extensively in   future courses at Berkeley and it's a standard  part of the computer science curriculum. Rather   than telling you all the definitions, which you're  welcome to look up on your own, I will show you   the standard notation that's associated with  those definitions, because it's used so commonly,   that it might surprise you if you've never seen  it before. So for these common orders of growth,   exponential, quadratic, linear, logarithmic, and  constant, the most typical way to describe each   order of growth is with either Big Theta or Big  O Notation. Big Theta notation looks like this.   And there's a different expression for each order  of growth. So quadratic growth is big theta of n   squared. Whereas, linear growth is big theta of  n. And yes, computer scientists really do wander   around saying big theta. Logarithmic is log n  and constant is the tricky one. You write big   theta of 1 most commonly, and what happened to the  end? Well, increasing n doesn't affect time in a   constant growth function, and so you don't put  n there at all. Another common set of notation   is big O notation, which is even more fun to say  than big theta. And the expressions look the same.   So big theta of b to the n, or big O of b to the  n. Now, these actually means something slightly   different. Big O describes the upper bound for  the time it takes for a function to run. So this   is like saying, at most quadratic. Whereas this  says something about a lower and upper bound,   both at most quadratic and at least quadratic  time is required. Now, to use this mathematical   notation effectively, you have to be quite  precise about what you're describing. What is n   in relation to the problem? If n is just the input  number, then things are simple. But n might be the   length of the input sequence. And that means there  are lots of sequences all with the same length. So   are you describing how long it takes for a length  in sequence in the worst case, or best case,   in average case, across all different sequences of  length n? That's something you'd want to specify   if you are going to go prove properties about  functions. And all that will become important   in later courses. But I think in this course,  the important thing is to recognize that there   are different general patterns of growth and the  time it takes for a function to complete. The most   common are the five that I've shown you here. And  when people discuss logarithmic growth, sometimes   they don't say logarithmic growth. Instead,  they say big theta log in or big O log in.
