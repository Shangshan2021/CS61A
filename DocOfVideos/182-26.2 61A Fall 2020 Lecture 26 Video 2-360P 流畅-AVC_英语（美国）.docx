we are all living through phenomenal digital revolution advances in technology and data acquisition and artificial intelligence have impacted just about every corner of our lives not just here in the u.s but around the globe to some 7 billion people and many of these things are wonderful there are really many tremendous things about democratization of access to information and to technology and bring the world closer in communication but as with every innovation there are dark sides and as a young budding computer scientist we shouldn't ignore those we should pay attention to the ethical issues and the potential harm that can come from technologies that we ourselves are developing and that's what this lecture is about today by way of a case study so i'm going to be talking a little bit about ethical issues around artificial intelligence and data and the work i'm going to be describing is that of julia dressler julia was a student with me at dartmouth college she graduated in 2017 and the work i'm going to be describing was her her senior thesis work and this work all started um around this issue of the use of ai to make recommendations now you have almost certainly been subjected to a recommendation that looks something like this if you liked x then you may like why netflix amazon spotify recommendations that look at the patterns of movies you watch products you buy music that you listen to look at what other people have done with those same patterns and make recommendations sometimes it works sometimes it doesn't either way it's fine i mean there's just recommendations for for watching movies now you may have also been subjected to a recommendation engine that looks something like this if you are like x then we may accept you uh companies in some cases universities are using automatic algorithms to review your applications and make determinations as to whether you should be interviewed or granted a job here the stakes are of course getting a little bit higher now because we are talking about your potential employment and educational prospects maybe you have not already because you're all young but many of us have been subjected to uh predictions that look like this if you are like axe then we may give you a loan uh banks financial institutions routinely look at your credit history your history in uh financial transactions compares that to other people looks to see what the default rates are and make recommendations do you get a mortgage do you get a small business loan stakes are getting a little bit higher now this is now your financial well-being as well uh and more and more over the last few years uh people have been subjected to the following type of prediction if you are like axe then we may put you in jail courts now around the country are routinely using predictive algorithms to determine whether you are at risk of committing a crime whether you're being released on probation whether you're being considered for bail and now the stakes are getting very high which is now it is your very liberty that is at stake for these prediction algorithms and it is these types of algorithms in particular that we're going to talk about today now uh in 2016 propublica wrote a really impactful uh piece and if you have not seen this i'm going to encourage you to search for this piece called machine bias their software used across the country to predict future criminals and it is biased against blacks it was a phenomenal body of reporting that pointed to some very troubling patterns in a software that was being used in the courts of law to make predictions about whether somebody is at risk of committing a crime in the future and whether that person should be granted bail or not so let me talk a little bit about what this article found and i'm going to tell you a little bit of that about what julia and i worked on for the following year to try to really understand what's happening with this type of machine bias so a pro publica found is that one particular software produced what's called the recidivism risk score this essentially takes his input and amount some information about you um how many times you've been committed committed to crime before how many times you've been arrested how many how much time you served in jail your employment history your age your gender and the long list of things that you might have information about somebody and it produced a score and that score was what is your risk of committing a crime in the next two years and then that could be used and in fact was being used by the courts to make a judgment as to whether you were at risk for recidivating and whether they should grant you bail and what propublica found is that the mistakes that this algorithm made and it's okay we understand that these things are not perfect but the nature of the mistakes were very problematic so i'm going to talk about two types of mistakes the first one is a false positive which says i'm going to predict that you recidivate but you in fact don't so i'm going to classify you as high risk but in fact you're not high risk at all and what they found is that if you were black then you were 44.9 of the times you were predicted to recidivate when you didn't but if you were right right that was only about half of that 23 and a half percent that is a huge difference um across the races so that is black people were being falsely accused of being high risk when in fact they were not at a rate twice that of white people now that's not the only type of mistakes that the algorithm makes it also can make a false negative which is that you are predicted not to recidivate but in fact you do so i say you know what this person's not high risk but it turns out they go out and they commit a crime so let's look at the error rate for these false negatives for across races if you were black that error only happened 28 percent of the time but if you were white it happened 47 47.7 percent of the time exactly the opposite so in the previous false positive blacks were at about a factor of two disadvantage of saying they were high risk when they weren't and not only that but in this mistakes white are being advantaged by saying that they are not high risk when they are so obviously this is deeply troubling um this is uh not obviously fair and we need to understand this if we're going to use these types of ai algorithms we need to understand where is this coming from and what can we do about it so shortly after this article appear julia came to me and said we have to do something about this we are technologists we are mathematicians we are computer scientists we have to fix this we have to build a better prediction algorithm and i agreed with her and we set out over the next year and a half or so to try to do that and as we were doing that a question came up we were sitting in my office one day and uh one of us asked the question is well you know we we keep trying to come up with algorithms that are better than humans but do we actually know what how good humans are at this task i mean just baseline i mean if you're trying to build an algorithm that deals with potential bias in the criminal justice system shouldn't we ask how well humans are and of course the answer is yes and so we went out and looked at the literature and in fact we couldn't find anybody who would assess what the baseline is so what are these algorithms being compared against how do you know whether an algorithm is improving the state of the art if you don't know how well humans are performing and so that's what we set out to do at the very beginning is just ask a really simple question is the ai these algorithms any better than you and me and potential people who are making these decisions and then we want to understand what is the nature of that racial bias that we saw and then can we fix it so we're going to start by answering this and then we're going to get to those other questions all right i'm going to stop right here and we'll pick it up in a little bit and start answering getting at this question 