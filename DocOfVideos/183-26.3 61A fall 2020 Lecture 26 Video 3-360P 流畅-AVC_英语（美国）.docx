where we left off was that we saw that commercial software that is being used in the courts seems to have a racial problem that is that these algorithms are more likely if you are black to say that you are at high risk um than if you are white and they are more likely to say that you are if you are white that you are low risk when you are not so there is this dual asymmetry it disproportionately affects black people by saying they're high risk and it disproportionately affects white people by saying that they are low risk when in fact they are not and as i mentioned julie and i wanted to start off by just asking a very basic question which is if you're going to use these algorithms surely we should understand where are we the human and making this judgment and then we can start getting at are these algorithms better how do you deal with the racial bias and the long list of questions we should think about when developing and deploying technology so let's start by answering this question so here's what we did uh we went on to a crowdsourcing website this is amazon's mechanical turk you pay people a couple of bucks and they answer a bunch of questions for you and you can collect thousands and thousands of these data within a matter of days and get a sense of how do people make these judgments now obviously these are not judges lawyers social workers people in the criminal criminal justice system these are just random people on the internet answering questions so surely this will be a lower bar for performance but we wanted to start there and then we'll see where we are now we need some data so we have got the people who are going to ask but now we need data so fortunately broward county florida has made public uh thousands of records anonymized of course of people um what their uh demographics are their race their gender their age uh their employment history their their criminal history um and then they track them for two years so at point of arrest we could ask based on the historical data where is this per what is the prediction and then we can follow that person for two years now obviously let me just state the obvious we don't know whether they committed a crime or not all we can know about is whether they were arrested and charged for a crime so that's what we mean by recidivism here okay so we've got data thousands of people we have where a number of data and let me show you what that data looks like so this is a snapshot of what we asked our mechanical turk users to do we didn't give them all the data associated with the people in that broward county database because it was just too much but here's what the information we gave them so we said that we gave him the gender the defendant is male age 43 they have been charged with disorderly intoxication and then down below here because some of these charges are not entirely obvious we would define what that charge is so now you know their their gender and their age and what the current charge is the crime is classified as a misdemeanor that's the fourth piece of data so misdemeanor felony et cetera and they have been convicted of two prior crimes uh and they have zero juvenile felony uh charges and zero juvenile misdemeanor so we tell you how many uh charges as an adult two and how many felony and misdemeanors as a juvenile obviously the felonies are the more serious charges as opposed to the misdemeanors and then we ask the person reading this short paragraph do you think that this person will commit another crime within two years and they answer yes no and we show them their current accuracy up here and to motivate them to do the task well we say that if you get above a 65 accuracy and you'll see why i picked that number in a minute then we will give you a bonus so we try to motivate them to you know actually do the task yeah all right so one two three four five six seven pieces of information and that is it and i'm going to ask you to uh determine to predict whether this person commits a crime so let's do a few um um and see see how you do the by the way notice that race doesn't come in here um we'll come back to that a little bit but we don't say what the race of the person is okay and that's intentional of course the defendant is a female age 29 they have been charged with aggravated assault with a deadly weapon here's the charge right here if you want to read it uh this crime is classified as a felony they have zero prior crimes they have zero juvenile felony charges and zero juvenile misdemeanors do you think this person will commit a crime within two years yes or no what do you think you've got to make the call do i let this person out on bail or not okay so in fact this person did not commit a crime in the next or we were not charged with the crime in the following two years let's just do a few more to give you an intuition for this this defendant is female age 52 they've been charged with theft this crime is a misdemeanor they've been convicted of six prior crimes they have zero juvenile felony and two juvenile misdemeanors um do you think they'll commit a crime in the next two years yes or no what do you think this person in fact did go on to commit a crime in the next two years all right one more male age 22 they've been charged with possession of cannabis 20 grams or less this was obviously before widespread legalization of marijuana this crime is classified as a misdemeanor they have two prior crimes zero felony juveniles zero uh juvenile misdemeanors what do you think yes or no and this is also a no and that's it uh so our users just read one after another they saw about 60 to 70 of these i don't remember the exact amount and how'd they do so we got hundreds and hundreds of responses the overall accuracy of the computer algorithm was 65.4 so 65 so figure about race forget about false positive false negative and all that just overall accuracy the algorithm's at 65 that's by the way why we put that as the target for users to get a bonus we wanted to see if we can reach the same accuracy as the computer algorithm our human respondents on average we're at 64 percent basically the same so reading just that short paragraph with seven pieces of information they were performing as well on average as the computerized software that was being used in the courts of law to help judges make these decisions now is the obvious oh uh interestingly uh if we do a little bit of averaging to see if there's wisdom in the crowd as it's called so instead of just relying on one person telling us what the answer is we we had a majority rules you can get a little bit of bump you go from 64 to 67 but they're really statistically almost no different than the computerized algorithms and the ai algorithms so it looks like the humans and the algorithms are doing about the same and so now let's get at the nature of the mistakes right this racial issue so the humans had a false positive and let me remind you what a false positive is is they predicted that they would recidivate but they don't so this is that you're essentially falsely accusing somebody being high risk when they are not so our subjects on average said that a black person would recidivate when they wouldn't at 37 percent they made a mistake and white's at 27 there's a racial bias here this is weird why is it weird they don't know the race of the person it's not in the data directly we don't provide it we give you the age and the gender so that's a little weird and about a little bit better than the software but not much better right so black people are being predicted at high risk incorrectly at a higher rate than white people let's look at the other mistake false negative i predict that you won't recidivate but you do i say you're a good guy go out and prosper but you go out and commit a crime we make that mistake humans make that mistake with blacks only 29 of the time but with weights 40 of the time so again you have this racial discrepancy and it's weird it's weird because there's no race in the data how are you biased against black people when i don't tell you what their race is that's strange and by the way i should tell you that when that propublica piece came out uh the software said this is impossible because we don't provide race into the predictive algorithm they don't either all right we're gonna have to get to the bottom of that and we will all right but what we thought is okay so something weird is going on let's just go ahead and rerun the experiment and tell them the race of the people maybe somehow by telling them the race we thought oh that's going to make them racially biased but maybe if we tell them it'll balance itself out so we added one more piece of information everything's the same the gender the age the the current charge prior crimes all that and then we just told you whether the person was white or black okay and then how did they do all right let's go see what happens there all right crowd accuracy without race we just saw they were at 67 percent with race they're at 66.5 they're basically the same so it makes no difference uh for the overall accuracy whether you know the person's race or not it's it's irrelevant and let's look at the mistakes false positive predict that you recidivate when you don't um i say that you're going to be high risk but you are not without race black we make a mistake with blacks at 37 white's at 27 there's the racial asymmetry if i tell you the race it's basically the same 40 26 a little bit of a bump here but not statistically different so whether you know the race or not you are biased against black people in terms of the false positive false negatives i predict that you will not recidivate when you do without race 29 to 40 percent 30 to 42 it's exactly the same thing so whether you know the race or not we seem to have a racial bias problem and that's weird how is that possible okay so a couple of things we need to get to how is it that making predictions about recidivism is not race blind when you don't know the race obviously we have a problem the software the humans we have there's a racial asymmetry here and we need to get to the bottom of that i would argue more fundamentally how is it that an ai algorithm that is being sold to our courts here in the united states and is being used by the courts to predict whether you should get bail or not um is as accurate as a random person on the internet being paid a couple bucks to answer a survey question from seven pieces of data how is that possible all right we need to get to the bottom of both of those questions and that's what we're gonna do in the next segment so i'll see in a few minutes 