to summarize where are we uh commercial software is not very accurate and has a race problem humans are the same uh we have a problem that we don't really understand why is it that a commercial software and humans are not race blind when they don't know the race of the defendant and how is it that commercial software and paying people a couple of bucks on the internet are giving us the same overall accuracy so where i left off in the previous segment was we really need to start to understand these algorithms if we're going to get answers to those questions we got to start by asking what do these algorithms do i can't probe the brain i'm not a neuroscientist and so but i can probe these algorithms and in the last segment i described to you a very very simple basic classifier for taking information and making a prediction about risk and so here's what we did we took all seven pieces of information and we built a classifier and we asked how well does it do okay and then what we said is well do we really need all seven of those what if i only had six what if i didn't know the sex for example or i didn't know the age or i didn't know the prior crimes and then we thought well okay what if we only need five what if you only need four pieces of data one of the only three two and one so we looked at all possible subsets of these seven pieces of information from the full set to all subsets of size 6 5 4 3 2 1. so classifier would one would only take the age and try to make a prediction only take the sex and try to make a prediction and then we asked what's the best you can do where is the best of all of those classifiers in terms of overall accuracy and then the hope is by looking at that classifier and what information it's latching onto we can get answers to our questions that have been plaguing us from the beginning of this talk and so here's what we found it turns out that if you feed only two pieces of information how old somebody is and their total number of prior crimes don't care about juvenile non felony misdemeanor sex don't care about any of that if i only have two pieces of information the classifier that we built the linear discriminant analysis the simplest possible classifier you can build was as accurate as the commercial software so this accuracy is the average accuracy of a commercial piece of software being sold to the courts to make predictions about defendants in the court of law this is something that julie and i hacked up in an afternoon right so that's interesting for a couple of reasons okay and we're going to get to that in a little bit here's the most interesting part of it so what i'm going to show you here are the two pieces of information that we are using in our classifier total number of prior crimes and age so again this is that two-dimensional space that i was talking about earlier on how old is somebody from young to old how many prior crimes do they have from very few to a lot and what this color coding is here is our prediction of risk yellow up here in the top corner is considered high risk and blue in the bottom right hand corner is considered low risk and this is the color coding of our classifier so once we have a classifier based on two things it's supervised learning we train it we can feed in any possible combination of age and prior crime and it'll make a prediction and then i just make that turn that prediction into a color code and i can visualize what the prediction space looks like and so what is it doing well let's see what it's saying is that if you are young and have a lot of prior crimes you're at high risk sure that sort of makes sense i guess and it's saying if you're old and have very few prior crimes you are low risk and of course if you're old and have a lot of prior crimes you're still at risk but less so than if you are young okay so that's what the classifier is doing and what's nice about our linear classifier is it's interpretable we can reason about it because we know the data coming in we were able to show that this was the best classifier that came out of it and we can look at this space and see or at least intuit what is the classifier doing that's very nice very positive okay so how does this help get in our questions it's not race blind when he doesn't know race this classifier doesn't know race and how is it that it's as good as uh paying somebody a couple of bucks on the internet so let's go to this one first so the problem of course is that prior crimes is a proxy for race we know that in this country for decades now if you are a person of color you are significantly more likely to be arrested charged and convicted of a crime with balancing for all other aspects that is a largely undisputed fact in the criminal justice system so when we were saying that the algorithms and the humans don't know about race that was strictly true but not entirely true because yes i didn't tell you this person's black or this person's white but i told you something that is a proxy for race because people of color again are more likely to be charged convicted charged uh arrested charged and convicted and therefore looking at the number of prior crimes is an uh a peak into their race again it's a proxy so this just the understanding that total number of crimes was this in addition to age was the single most important predictor gives us some insight into this race problem for both the algorithms and probably for the humans as well i mean there's really no other explanation if you don't know the race of the person and we know that humans are probably doing something very similar to this as well now let's get to this question how is it that ai and a random internet user are basically the same accuracy well the answer is that it's a really simple classifier so think back to when you were looking at those few and that's why i showed you those examples what were you doing right i mean if you saw somebody who was 45 and had zero prior convictions do you really think they're going to go out and commit a crime in the future does that seem reasonable to you or do you think they did something stupid and they got caught as opposed to a young kid who's 20 something and has 13 prior convictions that person clearly something is going on in their life and so basically the reason why these algorithms and the random internet user's name is that we're doing something very simple we intuit we believe we infer that basically what people are doing is saying the same thing our classifier is saying if you're young with a lot of prior convictions you're high risk if you're old with very few prior convictions you're low risk that explains two things in one why internet users in this ai are basically the same accuracy and also where the race the lack of race blindness comes in because this acts as prior crimes is a proxy for race all right you can say well who cares who cares that the ai and the random internet user are exactly the same i mean at least at least they're not worse and so why not use the ai and take the human out of the loop well i would argue however there's a little bit of a problem with that and that is because of the sense of authoritarian voice so what do i mean by that imagine the following scenario imagine you're a judge and uh you have a defendant before you and they're being asked uh to be released on bail and the court tells you hey we have this sophisticated ai system built on big data and the latest advances in deep learning um and we and it's commercial product we paid for this thing which obviously associates value with it and it's said that this person is high risk you would think the judge would at least give some credence to that yeah like that would sound authoritative now imagine a different scenario same defendant same question bail or nobel and uh the lawyer says well somebody on the internet tweeted that this person is high risk do you think the judge would care at all what a random person on the internet said of course not that's not authoritative what somebody on reddit or twitter or facebook said i don't care and yet the accuracies are exactly the same and the mistakes are the same and so when you when you wrap things around these ai boxes they seem authoritative and if they're authoritative you give more credibility to that even if it doesn't deserve it so i would i would argue it still matters even though that these accuracies are the same now here's a really interesting question is i remember i told you we built this really dumb simple classifier right so can we do better now we're going to get back to the original question that julie and i set out to do so all of this was in some ways like just you know what is going on here why are these things racially biased what is the baseline once we figure out the baseline we're like well why is there bias in the first place when they don't know race and i think that we've probably unraveled that we've gotten to the bottom of that it's an incredibly simple classifier and we have a proxy for race in terms of the total number of uh prior so i think we have a good understanding of now the space and now the question is can you get rid of bias and can you make the accuracies better okay so let me just talk very briefly about some of the work we've done on that so the first thing we asked is well what happens if we go from a linear classifier the simplest possible classifier you can build to a non-linear classifier and without going into the details of it a non-linear classifier basically allows you to build not just a line through the space but a curvy a curve that allows you to carve out exceptions right so here you can see for example i'm carving around here so i have more of the yellow points down below and more of the red points above so these are called non-linear classifiers support vector machines neural networks deep learning all of those fall into that category of non-linear classifier and just one non-linear classifier that we built was really really disappointing again this is the overall accuracy of the commercial software 65 our human 64 percent the linear classifier that the lda that we've already talked about was 66 and the non-linear was 65 they're all exactly the same we just seem to be stuck at around 65 percent okay i posit i cannot prove but i pause it i think that this is a fundamentally hard problem and i am unconvinced that you can actually do better than this because think about what you're asking the algorithm to do you're asking it to predict the future from a relatively small amount of data in the future two years in advance of a fairly complex set of socio-economic personal and just you know what is random dumb luck going to happen in somebody's life and i don't think that's a stretch of the imagination to say that this is really hard so here's a question for you should we even be doing this should we actually be trying to predict whether somebody's going to commit a crime in the future and then incarcerate them if we think that they are if the actor is 665 percent what if the accuracy is 75 what if it's 85 what if it's you what if it's somebody you love do you want this algorithm being applied to somebody with this kind of error rate what's an acceptable error rate are these things really better than humans how do you deal with the bias nobody has good answers to these things so here's a question now i come back to the title right just because you can do something doesn't mean you should and as you enter into what is undoubtedly an incredibly exciting time for us in terms of computation and ai and data and the impact that we can have on the world we have to start thinking about what are the negative aspects of what we are doing should we be trying to make these decisions and if we do the answer may be yes but then are they accurate are they fair do they disproportionately affect women people of color lgbtq community uh people who are not born in this country people who don't native speakers whatever it is we have to think about the consequences of that we've spent the last 20 years with the mantra of move fast and break things and while lots of good things have come from that some really bad things have come from this bias in algorithms for hiring bias and algorithms and healthcare bias for algorithms in financial sectors bias for algorithms the criminal justice system bias and facial recognition we've got to tread lightly here and what that means is you can't come at this after the fact you can't develop deploy and then debug on the fly this isn't a word processing software if you have a bug somebody loses a document this is the real world where you make a mistake and somebody's sitting in jail or somebody doesn't get a home loan or somebody doesn't get a small business loan or somebody doesn't get a job or go to the university we are impacting real people's lives with our algorithms and our data and if we don't understand these things we have the potential to do way more harm than we do good and so the free-for-all of the last two decades in my opinion should be over and i and i and i want to emphasize that i'm not anti-technology i'm not saying don't do things i'm not saying don't innovate but i'm saying think think carefully about the consequences of what you're doing and make sure that there is transparency there's fairness and there's accuracy and how these technologies are being used and more generally making sure that you understand how your technologies can be misused as well because almost all technologies have benefits and drawbacks and we have to start thinking about those things up front and simply try to mitigate the harm while harnessing the phenomenal power of technology and ai and data all right i'm done i hope you enjoyed this and i hope you learned something from it and 