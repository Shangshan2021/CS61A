lecture 13 q a yeah so the question here is what does it mean when we say 64-bit representation 32-bit representation 16-bit 8-bit etc etc so what it's referring to so typically where you'll see this referred to is when they'll talk about things like the color depth on your monitor right this is a big graphics uh terminology 64-bit graphics yeah so what is that referring to it's only referring to the the number of unique integers you can represent with a single number so for example if i have an 8-bit number i can represent 0-255 right 256 total numbers so when you have 8-bit graphics you only had 256 distinct colors when i went to 16-bit i now have 2 to the power 16 whatever that is 2 to the 32 2 to the 64 et cetera et cetera so when we say 64-bit representations it means how many you you distinct integer valued now programmatically you can go beyond that with these things called big numbers and there's ways of numerically representing big numbers the silliness of a lot of this is in the human visual system has fundamental limitations on how many distinct colors that it can represent by the way i have to make a disclosure my wife is a neuroscientist and a vision scientist and it always makes her crazy when i talk about the human brain and i think she's like lurking around here somewhere coming to talk quietly bring her around in you know her otherwise ah crap she heard me she's right there she looks pissed so the brain you know can't i mean so at some point the brain can't distinguish all those distinct colors and so that you know 64-bit hundreds it doesn't matter at some point yeah but to your question very specifically um when we say 64-bit we're saying how big is a single integer and that tells me how large of a number i can represent and then what i can do with that number like for example representing distinct colors on the monitor yeah and the other part of that question was why are there billions of transistors could we only need 64 well you want to represent lots of different numbers and you also want to represent the ability to perform operations on many of those numbers sometimes actually all at once so um the logic for performing some of the stuff that we'll see on wednesday like how you add two numbers together is actually built into the chip it's not like there's a program running in software that adds two numbers instead it's part of the circuitry of the hardware of your computer that's adding those two numbers together and whether that adder is built to operate on 32-bit integers or 64-bit integers is a decision that the hardware designers have to make and then the software has to all kind of take advantage of that so modern computers typically are built on 64-bit integers as their kind of like basic unit and that's like a fact about not how the memory is stored so much but about how the central processing unit manages all this information so if you want to like store a bunch of numbers and operate on all a bunch of them at the same time so that the computer could for example be adding these two numbers and these two numbers because you know you need to add them both so you might as well do them at the same time then all of a sudden you're getting a lot of transistors yeah it's a really good point and i i missed that part of your question so i apologize but yeah so what you're going to see is the transistors represent can be used to represent to store a number but it's also going to be used to do computation um and they are the very building blocks of both storing data and computing on that data and you'll see that i think in wednesday's lecture the next question was how is the arrangement of transistors determined are half of them in series and have them in parallel or something else so these are as john was just saying these are done beforehand so what what happens is i need to i need some special circuitry to add two things together and you don't know how to do that yet but you will have to wednesday and so i say okay how many and gates do i need how many objects do i need and i wire them in and then that's it it's not like we're on the fly rewiring these transistors to do various computations i think there was a time when that was sort of popular and i don't know if that still is anymore there was this notion of sort of dynamic um reconfiguring but i don't know if we but basically these things are hardwired um you decide what you need what what what computations do you want to be able to perform on your computer and you wire them in and then that's it and then you get to use that circuitry and then the you know the operating system and the controller says okay send this information over here there's a circuit to do that send this information over here there's a circuit to do that so once you decide what you want the nature of your computation is you figure out and you'll see what the pipeline is we will actually design circuitry i'll show you how to do addition and a few other things you and there's just a pipeline for doing it it's really quite straightforward actually in the simplest case and then you're like okay build give me an and gate give me an or gate give me a not gate give me this wire them up input here output here that's done that's a chip on your computer or on your phone or on whatever device it is yeah exactly and the those basic operations are there on the chip and if that's all that you wanted then you could have kind of a smaller chip than we have today but basically what's happened is that common combinations like addition or you know taking a logarithm or whatever happens so often that it makes more sense to build a circuit that's kind of especially for that in addition to the basic ones that just run and or not and those more complicated circuits aren't anything more than just like a bunch of and or knots all chained together in the right way but if you're going to do something over and over and over again and you kind of anticipate that it's going to happen then you can make it happen faster if you just like optimize it in the hardware in the circuitry in addition to having the full flexibility of just like combining any and ors of knots in any way that you want yeah yeah that's exactly right so you know no doubt the biggest impact on computing has been the miniaturization of transistors i mean that's been just a huge huge win uh for speed and power and compactness but also what happens is as computers are running as we figure out what are users spending more time doing we may decide we may realize oh users are spending a lot of time doing this one computation let's build a special circuitry let's put it in the right place and we can make computers uh five percent more faster great right so there's also a lot of algorithmic optimization that goes on not just hardware or optimization i just got the stink eye from my wife again by the way she just walked by [Laughter] the next question was have people build computers using base 10 instead of base 2 and would that offer some advantages yeah that's actually an interesting question is there would would there have been a benefit to base 10 like let's say we we didn't have to worry about the stability of a bit representation um well let's let's start by agreeing that there's nothing you can't do in base two the base that we work in is arbitrary yeah i mean there's there's no mathematics i can't do in base two or based on so that's an arbitrary decision based 10 is of course an arbitrary too well look uh simple things like multiplying and dividing by 10 are really easy in base 10.
um so that would be a lot faster but you know i could say the same thing about base two uh multiplying and dividing by two is really fast in base two so i i don't know i don't know if like would somehow if i had a base 10 computer would it be faster at certain things would there be a benefit to certain things off the top of my head i can't think of any can you john yeah no i don't think that there's any obvious advantage except that the computer does do a fair amount of converting like it needs to display numbers to the users and so at that point it needs to convert them to base 10 but that's just such a vanishingly small amount of the total computation that a normal program does that it's okay to have to do this conversion every time you want to print out a number and every time you want to interpret a number so if i type in a number i type it in in base 10 that has to be kind of then represented as base 2 and then all the math happens um but usually that math is just so much more work than either interpreting or displaying the number that it's okay to be switching bases um and yeah if if you wanna build circuits for base 10 they're just more stuff to build than in base two like in base two you kind of just have to memorize that zero plus zero is zero and zero plus one is one and one plus one well that's the special case where you carry uh but you'd have to do this for every possible combination of digits in base ten you'd have to remember that like three plus four is seven and kind of build that as a separate pathway so um one consequence of building a base ten computer i think would be that there would just be kind of more hardware to do the same thing and and so instead you could have more hardware to do kind of other stuff and that's the compromise that led everybody to using base 2.
yeah that's a good point the rules would get a lot more complicated one of the nice things about base 2 is the core rules for base 2 arithmetic are trivial whereas in base 10 they are a little bit more involved so maybe that would end up making the circuitry a little bit more complicated by the way i don't know if somebody has built a not i mean i'm sure somebody has built a non-base two computer um and there's all there was there was a time in the i think it was like in the 90s and the early 2000s where biological computing was very popular people were trying to do computing with dna and molecules and the idea was to use the power and the speed of uh nature to do computation it never there were these really influential papers for a few years big splashy signs and nature papers but they never quite got anywhere today of course quantum computing is has been a buzz now for well actually for quite a while um i think there's something personally i think there's something more real with quantum computing and here you're getting into a fundamentally new representation you are not representing with an actual zero one it's these it's the quantum state of a qubit and there is some indication that things that are slow on a traditional computer um could be very very fast on a quantum computer most interestingly is factoring large numbers which is interesting because that's the core of encryption so organizations like the nsa are very interested in this because if you can build at scale of quantum computing you can break passwords um and that you cannot do in a traditional computer so these new computing paradigms where you represent information differently i've been around for quite a while there's a whole field of optical computing where you represent information with light and that light moves through optical trains nothing really i mean they're all sort of interesting and insightful but in my opinion really it's quantum computing is the one thing that has has been slowly but surely trickling up john you may know more about this there's this canadian companies that claims to build the quantum computing but i i think it's controversial and i think many people have doubted whether it's a true quantum computing computer and so there's there's still some uncertainty about the real nature of what is feasible today with a different paradigm my understanding is that it was just last year that that someone claimed somewhat credibly for the first time that they had performed a computation that otherwise wouldn't have been easy to perform but they got to kind of choose the competition so instead of having this be like something that's generally useful at this point it's it's still kind of in proof of concept yeah and i thought there was still some controversy around that or at least i saw some controversy i don't know if it ever got resolved but i think there's something there and i think you know when that breaks through it's going to be a fundamental shift in the way we do confrontation is my guess yes and i believe that the kind of uh a national center for studying quantum computing was just designated here at berkeley so uh i think that's right one of the big we'll be hiring some new folks and uh it all might happen right here yeah i either by the way what's really beautiful about quantum computing is the combination of physics mathematics computer science electrical engineering this is incredible in chemistry incredibly complex from the physical to the theoretical and i think there's something really cool there but you know honey raised a great point about talking about kind of different ways of building computers and sort of what's the history and we're at a moment in time where the way we build things now may be very different from how we build things several years from now but the pattern that has emerged so far in computing with quantum computing be kind of an interesting counter example but the pattern that's emerged so far is that if we can just come up with one way of build of building computers that's really fast and really flexible then we can simulate anything else that we would want through software and through kind of the abstraction that you're learning about here so um it's very common that there will be kind of different ways of expressing programs than as a bunch of bit manipulations and yet instead of building a new piece of hardware to achieve that for the last several decades the answer has always been build a piece of software in order to simulate a machine that would have done that using kind of a regular computer and actually that's like very close to the nature of programming languages it's like programming language is really a way of expressing computation a lot of what you see in a modern programming language doesn't look like uh manipulating bits very much it kind of looks like uh you know creating functions and calling them and recursion and all of that you could try to build a machine specifically to do those things but instead we kind of simulate that using the hardware that we have that happens to run really fast yeah that's a really good point john that's a great point that that has not changed and i don't see that changing either sure so i think this was from friday's lecture what's going on with the min function and its optional argument called key sometimes what you want is to find the smallest number among a list and min will do that for you but sometimes what you want is to find a particular number that's extreme in another way like it's not the smallest it's not the largest but it's the closest to five or it's the it's the thing that when you square it is closest to 24.
or you know you could imagine any kind of description where there's like a some value that would do this the best and min allows you to find that element for any possible condition and that's the point of that key function so the way you do it is that you start out with your same set of values of which one is the one that you're looking for and then you provide a function and this function is going to be called on every single one of these values and yeah let's use that example of uh the square is as close as possible to 24. i don't know why you would want this but maybe you would for some reason so what's going to happen here is that it's going to square and then subtract 24 from each of these and then take the absolute value in order to get some measurement of how close is the square of this x to 24 and it tells us 5 is among these numbers the one that when you square it gets you pretty close to 24.
in fact there's like an important property about this computation that's not shown which is that 5 squared minus 24 is 1 and there's no 1 in this output that's all hidden what is happening is that that 1 is computed along with the result of squaring 3 and subtracting 24 and squaring 6 and subtracting 24. so it's done it for all of them and then it has found the one for which the result of calling this function is the smallest that's why the smallest because it's min so in this way you can use min in order to find the kind of extreme value for any notion of extreme that you want but you always get one of these values and if there's a tie then you'll get an arbitrary one i think you get the first one but i'm not actually sure so another way to think about it is take that list apply the transformation that you see in the lambda expression square every value subtract 24 and then just call min right you could it's just a shortcut for doing something in a clumsy in a more clumsy way yeah and the reason it's so useful is that let's say you did this uh went through and computed f of x for every x in this list you get all these numbers but what you really want is the thing that corresponds to this so you really want like to remember the x that goes with it oh there's the smallest one so then if i took the min of this i'd get the pair that i want and then i could pluck out the value that got me there and this would like do the same thing but wow that's a mess so instead of writing it this way uh it's more common to write it that way and there's a couple of questions in the chat so let me go through them one at a time does the key function always have to be a lambda function i believe it does uh no any function could could do so sorry yes it could be any but it has to be a function sorry yes it doesn't have to be a lambda but you can call um [Music] um square root of x right guys well let's see yeah exactly so uh right if we wanted like what's the min from negative 5 and 4 and negative 7 and 8. well the min is negative 7 unless you say oh actually i want to take the absolute value of all of them first and then it would find the 4. good and also what if you wanted the key function to take multiple parameters that i don't think you can do so i think it has to operate point wise on the element on the elements of the list that's right so if if you had some function of two arguments that said return the absolute difference between x squared and y let's say you wouldn't be able to use that as a key because it takes two arguments and the whole idea is that we're going to call this function on each one of these individually now you might do something like create a one argument function that calls f on x and 24 and like fill in the y that way and then that would work but uh otherwise you have to have a function here that's of one argument okay uh uh there's a question here when you took the min of a list of lists what determined what the minimum was um so i think we're referring to this line yeah so we had this thing yes and to find the minimum you'd have to like compare that to that and it's weird because this is smaller than that but this is smaller than that so which one is it well this is something that just happens to be defined in python is that when you're comparing two lists you compare their first elements and if one of them's smaller then it uses that one by the way i think that's ridiculous i mean that's i just got there's a lot of things that the boolean operators that i've always found slightly annoying about python but that one always bugs me yeah okay um okay did we get your um i think tan seems to have loaded yeah you go tan yeah thank you ah uh at the end of the min has a square uh has square bracket one mean the first element yes well it's the what's in the position one so it's the second element yeah so in this monstrosity wait what have i done oh i changed f ah right oh my goodness all right there was an old f uh so in this monstrosity we evaluated this expression to get a pair and like a sensible thing would be to give this a name and then p1 is the 5 and p0 is the 1 and there's no p2 because it's 